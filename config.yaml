---
###############################################################################
# credentials

# The credentials path containing tokens for the twitter and locationiq API
credentials_path: "credentials.yaml"

###############################################################################
# tweets filter

# period of time between two processing of raw tweets
time_interval_process: 60
# Directory path of the raw tweets generated by stream.py
raw_tweets_stream_dir_path: "raw_tweets_stream"
# Directory path of the raw tweets generated by search_users.py
raw_tweets_search_dir_path: "raw_tweets_search"
# Directory path where to store the swiss-german tweets found
out_dir_tweet_processing: "out_process"
# Path where to store the final list of Swiss-German sentences, along with
# the full tweet object
path_dirty_gsw_tweets: "dirty_dataset/gsw_tweets.pkl"
# Path where to store the final dataset containing all Swiss-German sentences.
path_dirty_gsw_sentences: "dirty_dataset/gsw_sentences.csv"
# Path of the file containing the ids of the tweets already processed
processed_tweets_ids_path: "data/processed_ids.txt"
# Path of the file containing the last tweet for each Swiss-German twitter user
sg_users_last_path: "data/sg_users_last.csv"
# Path of the file containing the count of gsw sentences for each user
sg_users_count_path: "data/sg_users_count.csv"
# Threshold used for Swiss-German language identification using BERT
lid_threshold: 0.9
# minimum size of words containing only special chars to be removed
min_char_special_group: 2

# Max special chars a word can contain. If the limit is exceeded, the whole
# sentence is dropped.
max_special_char_in_word: 3
# Threshold of the bert lid used for adding new Swiss-German twitter users.
# This value should be higher than the preceding, because we don't want to add a
# user if we are not sure he's Swiss-German.
threshold_new_sg_user: 0.995
# The index of the gpu to use (default is 0)
gpu_index_to_use: 0
# Keep location that are spotted outside of Switzerland
keep_foreign_location: true
# The minimum size of the location text field of twitter user account
# Not used anymore, we keep all gsw tweets
min_location_length: 1

# Specific twitter preprocessing, first elem is the pattern to find and second
# elem is by what we want to replace the pattern. Note that this only to keep
# well-formed swiss-german sentences, but the raw tweets is kept along with the
# sentence, so we don't loose information (e.g. we can still analyse the
# hashtags later)
preprocessing_regex:
    -
        - '^RT\s' # Retweet flag at the beginning of the tweet
        - ''
    -
        - '^MT\s' # Modified flag at the beginning of the tweet
        - ''
    -
        - '@\S*($|\s)' # Mentions
        - ''
    -
        - '#\S*($|\s)' # Hashtags
        - ''
    -
        - 'https?[\w\.\:\/]*($|\s)' # Urls
        - ''

###############################################################################
# geocoder

# Path of the text file mapping text (user.location) to locationiq objects or
# coordinates. This is to avoid calling locationiq with the same query more than
# once, which is very inefficient because of rate limitations.
loc_to_coords_path: "data/loc_to_coords.txt"

# Path of the Switzerland key-word locations. This contains city and states name
ch_words_path: "data/ch_words.txt"

# Polygon roughly describing Switzerland. Points are [longitude, latitude]
ch_polygon:
    -
        - 7.016304
        - 45.826467
    -
        - 6.582344
        - 47.007943
    -
        - 6.977852
        - 47.507519
    -
        - 8.559883
        - 47.829348
    -
        - 9.683268
        - 47.528088
    -
        - 9.650309
        - 47.075490
    -
        - 10.534708
        - 47.011854
    -
        - 10.496256
        - 46.526499
    -
        - 10.114481
        - 46.200498
    -
        - 9.482907
        - 46.298884
    -
        - 9.433468
        - 46.461834
    -
        - 9.288285
        - 46.449858
    -
        - 9.316437
        - 46.316282
    -
        - 9.021244
        - 45.798455
    -
        - 8.246708
        - 46.299792
    -
        - 8.178043
        - 46.153484
    -
        - 7.864933
        - 45.905585

###############################################################################
# corpus_32 and corpus_8

# path of the csv files containing the approximate number of speaker for each
# language. This is to ponderate the score of language-specific words.
path_speakers: "data/speakers.csv"
# directory path containing the leipzig corpus of 32 languages
corpus_32_dir_path: "data/leipzig_32"
# directory path containing the leipzig corpus of 8 languages
# (GSW, German, and 6 GSW-like languages). This can be same as
# corpus_32_dir_path because it is a subset of the 32 languages.
corpus_8_dir_path: "data/leipzig_32"

###############################################################################
# streamer

# log path
dir_path_log: "log"
# time interval (s) to wait before processing raw tweets and fetching users
time_interval_search_users: 86400
# tweet count to write in a single file when streaming
raw_tweets_stream_batch_size: 5000
# tweet count to write in a single file when searching for user's tweets
raw_tweets_search_batch_size: 4000
# gsw tweet count threshold (i.e. how much gsw tweet a user needs to have
# tweeted) to fetch all tweets from a given user
gsw_tweet_count_threshold: 1

# filter_languages will be used as a parameter of the filter function from the
# twitter api (for streaming tweet). It is a list of languages on which to
# filter.
filter_languages:
    - "de"
# Swiss-german corpus path, will be used to compute the most common words in
# swiss-german
gsw_corpus_path: "data/leipzig_32/gsw_swisstext_leipzig.txt"
# the directory path that contains specific swiss-german words
# the data in this directory is generated automatically, according to the next
# variables
track_word_dir_path: "data/track_words"
# choose whether we want the most common words, or the most specific words
# in case of the most specific words, TF-IDF is used on 32 languages
# and we keep the words where the gsw TF-IDF score is the highest, providing the
# word has enough specificity (see track_word_specificity)
track_word_type: "proportion" # either "common" or "tfidf" or "proportion"
# the count of common words to get (400 is the maximum for twitter queries)
track_word_count: 400
# which language we want to compute the most common words
track_word_language: "gsw"
# the specificity is only used when track_word_type="specific". It represents
# the ration of gsw TF-IDF score over the sum of TF-IDF score of every other
# languages. This is useful because if we simply take the highest TF-IDF score
# for Swiss-German, then we end up with very specific swiss-german words that
# are rarely used, so it will not allow a high coverage of swiss-german tweets
# This a tradeoff between GSW tweet coverage and gsw specific words.
# If the specificity is too high, then coverage is low
# If the specifitiy is too low, then a lot of non swiss-german tweets are
# listened.
track_word_specificity: 0.3
# Add a ponderation to the TF-IDF score corresponding to the number of speakers
# for each language. Motivation : suppose a word very representative of both
# Swiss-German and a language with 100k speakers. It may have a low TF-IDF score
# because it is share with another language, but since this other language has
# a low count of speakers, we don't really care because there will be very few
# tweets in this language. On the other hand, if a word is shared with english,
# then we may be drown in english tweets. The ponderation helps to avoid this
# scenarios.
track_word_ponderated: true

# Parameters used when querying the locationIQ API
locationiq:
    host: "https://eu1.locationiq.com/v1" # default

    # str | Format to geocode. Only JSON supported for SDKs
    gformat: 'json'

    # int | For responses with no city value in the address section,
    # the next available element in this order - city_district,
    # locality, town, borough, municipality, village, hamlet, quarter,
    # neighbourhood - from the address section will be normalized to
    # city. Defaults to 1 for SDKs.
    normalizecity: 1

    # int | Include a breakdown of the address into elements.
    # Defaults to 0. (optional)
    addressdetails: 1

    # str | The preferred area to find search results.
    # To restrict results to those within the viewbox, use along with
    # the bounded option. Tuple of 4 floats. Any two corner points of
    # the box - `max_lon,max_lat,min_lon,min_lat` or `min_lon,min_lat,
    # max_lon,max_lat` - are accepted in any order as long as they span
    # a real box.  (optional)
    # viewbox: ['15.22, 54.95, 5.55, 45.74'] # CH and DE only
    # viewbox: ['17.24, 54.95, 2.27, 45.74'] # German speaking coutries
    viewbox:

    # int | Restrict the results to only items contained with the
    # viewbox (optional)
    bounded: 1

    # int | Limit the number of returned results. (optional)
    # (default to 10)
    limit: 10

    # str | Preferred language order for showing search results,
    # overrides the value specified in the Accept-Language HTTP header.
    # Defaults to en. To use native language for the response when
    # available, use accept-language=native (optional)
    accept_language: 'en'

    # str | Limit search to a list of countries. (optional)
    # "ch,de,at,be,li,lu" # german speaking countries
    countrycodes: "ch"

    # int | Include a list of alternative names in the results.
    # These may include language variants, references, operator and
    # brand. (optional)
    namedetails: 1

    # int | Sometimes you have several objects in OSM identifying the
    # same place or object in reality. The simplest case is a street
    # being split in many different OSM ways due to different
    # characteristics. Nominatim will attempt to detect such duplicates
    # and only return one match; this is controlled by the dedupe
    # parameter which defaults to 1. Since the limit is, for reasons of
    # efficiency, enforced before and not after de-duplicating, it is
    # possible that de-duplicating leaves you with less results than
    # requested. (optional)
    dedupe: 1

    # int | Include additional information in the result if available,
    # e.g. wikipedia link, opening hours. (optional)
    extratags: 0

    # int | Adds state or province code when available to the statecode
    # key inside the address element. Currently supported for addresses
    # in the USA, Canada and Australia. Defaults to 0 (optional)
    statecode: 0

    # int | Returns additional information about quality of the result
    # in a matchquality object. Read more Defaults to 0 [0,1] (optional)
    matchquality: 0

    # int | Returns address inside the postaladdress key, that is
    # specifically formatted for each country. Currently supported for
    #addresses in Germany. Defaults to 0 [0,1] (optional)
    postaladdress: 0

...
